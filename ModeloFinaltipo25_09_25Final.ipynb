{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7gT0QnbGnIxQ",
        "outputId": "f4c3ddaa-8bf9-4ca9-c23a-ff1c1f246123"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.12/dist-packages (3.0.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.0.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.27.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from xgboost) (1.16.2)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.12/dist-packages (4.6.0)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from lightgbm) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from lightgbm) (1.16.2)\n",
            "Collecting catboost\n",
            "  Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from catboost) (0.21)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from catboost) (3.10.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.0.2)\n",
            "Requirement already satisfied: pandas>=0.24 in /usr/local/lib/python3.12/dist-packages (from catboost) (2.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from catboost) (1.16.2)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (from catboost) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from catboost) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=0.24->catboost) (2025.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (4.60.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->catboost) (3.2.4)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly->catboost) (8.5.0)\n",
            "Downloading catboost-1.2.8-cp312-cp312-manylinux2014_x86_64.whl (99.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: catboost\n",
            "Successfully installed catboost-1.2.8\n"
          ]
        }
      ],
      "source": [
        "!pip install xgboost\n",
        "!pip install lightgbm\n",
        "!pip install catboost"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install lime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G07MSsz0RpLh",
        "outputId": "09635ea9-970b-4fd6-be4c-647fd22202a0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lime\n",
            "  Downloading lime-0.2.0.1.tar.gz (275 kB)\n",
            "\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/275.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m275.7/275.7 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from lime) (3.10.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from lime) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from lime) (1.16.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from lime) (4.67.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.12/dist-packages (from lime) (1.6.1)\n",
            "Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.12/dist-packages (from lime) (0.25.2)\n",
            "Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.12->lime) (3.5)\n",
            "Requirement already satisfied: pillow>=10.1 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.12->lime) (11.3.0)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.12->lime) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.12->lime) (2025.9.9)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.12->lime) (25.0)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.12/dist-packages (from scikit-image>=0.12->lime) (0.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.18->lime) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn>=0.18->lime) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->lime) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->lime) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->lime) (4.60.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->lime) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->lime) (3.2.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->lime) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->lime) (1.17.0)\n",
            "Building wheels for collected packages: lime\n",
            "  Building wheel for lime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for lime: filename=lime-0.2.0.1-py3-none-any.whl size=283834 sha256=3f5ba554be3cbe1159ae80e8ed9061074798b47c17434761cb83a287b41200b5\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/5d/0e/4b4fff9a47468fed5633211fb3b76d1db43fe806a17fb7486a\n",
            "Successfully built lime\n",
            "Installing collected packages: lime\n",
            "Successfully installed lime-0.2.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Codigo para compartir**"
      ],
      "metadata": {
        "id": "7Eb-XjIvFSU_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#########################################################################################"
      ],
      "metadata": {
        "id": "f6vkmiOkmmTu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================\n",
        "# VECM con train-test split, pron√≥sticos y m√©tricas\n",
        "# ===========================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "from statsmodels.tsa.vector_ar.vecm import coint_johansen, VECM\n",
        "\n",
        "# ===========================\n",
        "# 1. Cargar y preparar datos\n",
        "# ===========================\n",
        "df = pd.read_excel(\"baseVECMfinal.xlsx\")\n",
        "\n",
        "tcr = \"ITCER\"\n",
        "fundamentales = [\n",
        "    \"IPC\", \"energia\", \"agua\", \"gasliquido\",\n",
        "    \"X\", \"M\", \"RIN\",\n",
        "    \"activa\", \"pasivaahorro\", \"pasivafijo\", \"libor3\", \"FEDFUNDS\",\n",
        "    \"EGRESOSCORRIENTES\", \"INGRESOSCORRIENTES\", \"EGRESOSCAPITAL\", \"INGRESOSCAPITAL\",\n",
        "    \"Oro\", \"Petroleo1\", \"Zinc\", \"Plata\", \"Estano\",\n",
        "    \"temperatura\", \"precipitation\", \"drought\"\n",
        "]\n",
        "\n",
        "model_df = df[[tcr] + fundamentales].dropna()\n",
        "\n",
        "# Variables que van en log\n",
        "log_vars = [\"ITCER\",\"IPC\",\"energia\",\"agua\",\"gasliquido\",\"X\",\"M\",\"RIN\",\n",
        "            \"Oro\",\"Petroleo1\",\"Zinc\",\"Plata\",\"Estano\"]\n",
        "\n",
        "for var in log_vars:\n",
        "    model_df[\"ln_\" + var] = np.log(model_df[var])\n",
        "\n",
        "# Dataset final\n",
        "Y = model_df[[\"ln_ITCER\",\"ln_IPC\",\"ln_X\",\"ln_M\",\"ln_RIN\",\"ln_Oro\",\"ln_Petroleo1\",\n",
        "              \"ln_Zinc\",\"ln_Plata\",\"ln_Estano\",\"activa\",\"pasivaahorro\",\"pasivafijo\",\n",
        "              \"libor3\",\"FEDFUNDS\",\"EGRESOSCORRIENTES\",\"INGRESOSCORRIENTES\",\n",
        "              \"EGRESOSCAPITAL\",\"INGRESOSCAPITAL\",\"temperatura\",\"precipitation\",\"drought\"]]\n",
        "\n",
        "# ===========================\n",
        "# 2. Train-test split\n",
        "# ===========================\n",
        "train_size = int(len(Y) * 0.8)\n",
        "train, test = Y.iloc[:train_size], Y.iloc[train_size:]\n",
        "\n",
        "# ===========================\n",
        "# 3. Prueba de cointegraci√≥n en train\n",
        "# ===========================\n",
        "johansen_test = coint_johansen(train, det_order=0, k_ar_diff=2)\n",
        "print(\"Trace test:\", johansen_test.lr1)\n",
        "print(\"Critical values:\", johansen_test.cvt)\n",
        "\n",
        "# ===========================\n",
        "# 4. Estimaci√≥n VECM en train\n",
        "# ===========================\n",
        "vecm = VECM(train, k_ar_diff=2, coint_rank=1, deterministic=\"co\")\n",
        "vecm_res = vecm.fit()\n",
        "print(vecm_res.summary())\n",
        "\n",
        "# ===========================\n",
        "# 5. Pron√≥sticos\n",
        "# ===========================\n",
        "# Pron√≥stico en horizonte de test\n",
        "n_test = len(test)\n",
        "forecast_test = vecm_res.predict(steps=n_test)\n",
        "forecast_test_df = pd.DataFrame(forecast_test,\n",
        "                                index=test.index,\n",
        "                                columns=Y.columns)\n",
        "\n",
        "# Pron√≥stico extendido 78 pasos adelante\n",
        "forecast_78 = vecm_res.predict(steps=78)\n",
        "forecast_78_df = pd.DataFrame(forecast_78,\n",
        "                              columns=Y.columns)\n",
        "\n",
        "# ===========================\n",
        "# 6. Guardar en Excel\n",
        "# ===========================\n",
        "with pd.ExcelWriter(\"pronosticos_VECM.xlsx\") as writer:\n",
        "    forecast_test_df.to_excel(writer, sheet_name=\"Forecast_Test\")\n",
        "    forecast_78_df.to_excel(writer, sheet_name=\"Forecast_78\")\n",
        "\n",
        "# ===========================\n",
        "# 7. M√©tricas en variable clave (ln_ITCER)\n",
        "# ===========================\n",
        "aligned = pd.concat([test[\"ln_ITCER\"], forecast_test_df[\"ln_ITCER\"]], axis=1).dropna()\n",
        "aligned.columns = [\"y_true\", \"y_pred\"]\n",
        "\n",
        "y_true = aligned[\"y_true\"].values\n",
        "y_pred = aligned[\"y_pred\"].values\n",
        "\n",
        "# M√©tricas\n",
        "mse = mean_squared_error(y_true, y_pred)\n",
        "rmse = np.sqrt(mse)  # corregido\n",
        "mae = mean_absolute_error(y_true, y_pred)\n",
        "\n",
        "with np.errstate(divide='ignore', invalid='ignore'):\n",
        "    mape = np.mean(np.abs((y_true - y_pred) / y_true))\n",
        "\n",
        "r2 = r2_score(y_true, y_pred)\n",
        "\n",
        "print(f\"R¬≤: {r2:.4f}\")\n",
        "print(f\"RMSE: {rmse:.6f}\")\n",
        "print(f\"MSE: {mse:.6f}\")\n",
        "print(f\"MAE: {mae:.6f}\")\n",
        "print(f\"MAPE: {mape*100:.4f}%\")\n",
        "\n",
        "# ===========================\n",
        "# 8. Gr√°fico comparativo\n",
        "# ===========================\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(train.index, train[\"ln_ITCER\"], label=\"Train\", color=\"blue\")\n",
        "plt.plot(aligned.index, aligned[\"y_true\"], label=\"Test Real\", color=\"black\")\n",
        "plt.plot(aligned.index, aligned[\"y_pred\"], label=\"Pron√≥stico\", linestyle=\"--\", color=\"red\")\n",
        "plt.title(\"Pron√≥stico VECM vs Valores Reales (ln_ITCER)\")\n",
        "plt.xlabel(\"Tiempo\")\n",
        "plt.ylabel(\"ln_ITCER\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "0HobbvuV93hQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===========================\n",
        "# ECM (Regresi√≥n Lineal) con Train-Test, Pron√≥sticos y M√©tricas\n",
        "# ===========================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# ===========================\n",
        "# 1. Cargar y preparar datos\n",
        "# ===========================\n",
        "df = pd.read_excel(\"baseVECMfinal.xlsx\")\n",
        "\n",
        "tcr = \"ITCER\"\n",
        "fundamentales = [\n",
        "    \"IPC\", \"energia\", \"agua\", \"gasliquido\",\n",
        "    \"X\", \"M\", \"RIN\",\n",
        "    \"activa\", \"pasivaahorro\", \"pasivafijo\", \"libor3\", \"FEDFUNDS\",\n",
        "    \"EGRESOSCORRIENTES\", \"INGRESOSCORRIENTES\", \"EGRESOSCAPITAL\", \"INGRESOSCAPITAL\",\n",
        "    \"Oro\", \"Petroleo1\", \"Zinc\", \"Plata\", \"Estano\",\n",
        "    \"temperatura\", \"precipitation\", \"drought\"\n",
        "]\n",
        "\n",
        "model_df = df[[tcr] + fundamentales].dropna()\n",
        "\n",
        "# Variables en log\n",
        "log_vars = [\"ITCER\",\"IPC\",\"energia\",\"agua\",\"gasliquido\",\"X\",\"M\",\"RIN\",\n",
        "            \"Oro\",\"Petroleo1\",\"Zinc\",\"Plata\",\"Estano\"]\n",
        "\n",
        "for var in log_vars:\n",
        "    model_df[\"ln_\" + var] = np.log(model_df[var])\n",
        "\n",
        "# Dataset: dependiente + regresores\n",
        "Y = model_df[\"ln_ITCER\"]\n",
        "X = model_df.drop(columns=[tcr, \"ITCER\", \"ln_ITCER\"])  # quitamos duplicados\n",
        "\n",
        "# ===========================\n",
        "# 2. Train-test split\n",
        "# ===========================\n",
        "train_size = int(len(X) * 0.8)\n",
        "X_train, X_test = X.iloc[:train_size], X.iloc[train_size:]\n",
        "Y_train, Y_test = Y.iloc[:train_size], Y.iloc[train_size:]\n",
        "\n",
        "# ===========================\n",
        "# 3. Estimaci√≥n de regresi√≥n lineal\n",
        "# ===========================\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, Y_train)\n",
        "\n",
        "# ===========================\n",
        "# 4. Pron√≥sticos\n",
        "# ===========================\n",
        "Y_pred = model.predict(X_test)\n",
        "\n",
        "# ===========================\n",
        "# 5. M√©tricas\n",
        "# ===========================\n",
        "mse = mean_squared_error(Y_test, Y_pred)\n",
        "rmse = np.sqrt(mse)\n",
        "mae = mean_absolute_error(Y_test, Y_pred)\n",
        "r2 = r2_score(Y_test, Y_pred)\n",
        "\n",
        "with np.errstate(divide='ignore', invalid='ignore'):\n",
        "    mape = np.mean(np.abs((Y_test.values - Y_pred) / Y_test.values))\n",
        "\n",
        "print(f\"R¬≤: {r2:.4f}\")\n",
        "print(f\"RMSE: {rmse:.6f}\")\n",
        "print(f\"MSE: {mse:.6f}\")\n",
        "print(f\"MAE: {mae:.6f}\")\n",
        "print(f\"MAPE: {mape*100:.4f}%\")\n",
        "\n",
        "# ===========================\n",
        "# 6. Guardar en Excel\n",
        "# ===========================\n",
        "results_df = pd.DataFrame({\n",
        "    \"Real\": Y_test.values,\n",
        "    \"Pronosticado\": Y_pred\n",
        "}, index=Y_test.index)\n",
        "\n",
        "with pd.ExcelWriter(\"pronosticos_ECM.xlsx\") as writer:\n",
        "    results_df.to_excel(writer, sheet_name=\"Forecast_Test\")\n",
        "    pd.DataFrame({\n",
        "        \"R2\":[r2], \"RMSE\":[rmse], \"MSE\":[mse], \"MAE\":[mae], \"MAPE\":[mape]\n",
        "    }).to_excel(writer, sheet_name=\"Metrics\", index=False)\n",
        "\n",
        "# ===========================\n",
        "# 7. Gr√°fico comparativo\n",
        "# ===========================\n",
        "plt.figure(figsize=(12,6))\n",
        "plt.plot(Y_train.index, Y_train, label=\"Train\", color=\"blue\")\n",
        "plt.plot(Y_test.index, Y_test, label=\"Test Real\", color=\"black\")\n",
        "plt.plot(Y_test.index, Y_pred, label=\"Pron√≥stico\", linestyle=\"--\", color=\"red\")\n",
        "plt.title(\"Regresi√≥n Lineal (ECM) - Pron√≥stico vs Real (ln_ITCER)\")\n",
        "plt.xlabel(\"Tiempo\")\n",
        "plt.ylabel(\"ln_ITCER\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "pheAkpuL_ANQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================\n",
        "# Predicci√≥n con Modelos - Versi√≥n Mejorada\n",
        "# Incluye: Validaci√≥n temporal, SHAP, LIME, IC para el mejor modelo\n",
        "# ============================\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from datetime import datetime\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# =======================\n",
        "# 1. Cargar y preparar datos\n",
        "# =======================\n",
        "file_path = \"nueva_base_con_rezagosfinal.xlsx\"\n",
        "df = pd.read_excel(file_path, sheet_name=\"Sheet1\")\n",
        "\n",
        "print(\"=== INFORMACI√ìN DE LOS DATOS ===\")\n",
        "print(f\"Dimensi√≥n original: {df.shape}\")\n",
        "\n",
        "# Eliminar filas con valores nulos en la variable objetivo\n",
        "df = df.dropna(subset=[\"ITCRM\"])\n",
        "print(f\"Dimensi√≥n despu√©s de limpiar NaN: {df.shape}\")\n",
        "\n",
        "# Crear √≠ndice temporal\n",
        "if 'fecha' in df.columns or 'Fecha' in df.columns:\n",
        "    date_col = 'fecha' if 'fecha' in df.columns else 'Fecha'\n",
        "    df[date_col] = pd.to_datetime(df[date_col])\n",
        "    df = df.sort_values(date_col)\n",
        "    df.index = df[date_col]\n",
        "    print(f\"Usando columna de fecha: {date_col}\")\n",
        "else:\n",
        "    # Crear √≠ndice temporal autom√°tico\n",
        "    dates = pd.date_range(start='1993-08-01', periods=len(df), freq='M')\n",
        "    df.index = dates\n",
        "    print(\"√çndice temporal creado autom√°ticamente\")\n",
        "\n",
        "print(f\"Rango temporal: {df.index.min()} a {df.index.max()}\")\n",
        "\n",
        "# =======================\n",
        "# 2. An√°lisis de estacionariedad\n",
        "# =======================\n",
        "from statsmodels.tsa.stattools import adfuller\n",
        "\n",
        "def test_estacionariedad(serie, nombre=\"Serie\"):\n",
        "    \"\"\"Test de Dickey-Fuller aumentado para estacionariedad\"\"\"\n",
        "    result = adfuller(serie.dropna())\n",
        "    p_value = result[1]\n",
        "    es_estacionaria = p_value < 0.05\n",
        "\n",
        "    print(f\"\\n{nombre}:\")\n",
        "    print(f\"  p-value: {p_value:.6f}\")\n",
        "    print(f\"  Estacionaria: {'S√ç' if es_estacionaria else 'NO'}\")\n",
        "    return es_estacionaria\n",
        "\n",
        "# Test de estacionariedad\n",
        "y_original = df[\"ITCRM\"]\n",
        "es_estacionaria = test_estacionariedad(y_original, \"ITCRM Original\")\n",
        "\n",
        "if not es_estacionaria:\n",
        "    print(\"\\n‚ö†Ô∏è  Aplicando diferenciaci√≥n para lograr estacionariedad...\")\n",
        "    y_transformada = y_original.diff().dropna()\n",
        "    es_estacionaria_transformada = test_estacionariedad(y_transformada, \"ITCRM Diferenciada\")\n",
        "\n",
        "    if es_estacionaria_transformada:\n",
        "        y_final = y_transformada\n",
        "        metodo = \"diferenciada\"\n",
        "        print(\"‚úÖ Usando serie diferenciada (estacionaria)\")\n",
        "    else:\n",
        "        y_final = y_original\n",
        "        metodo = \"original\"\n",
        "        print(\"‚ö†Ô∏è  Usando serie original (no estacionaria)\")\n",
        "else:\n",
        "    y_final = y_original\n",
        "    metodo = \"original\"\n",
        "    print(\"‚úÖ Usando serie original (estacionaria)\")\n",
        "\n",
        "# =======================\n",
        "# 3. Preparar variables predictoras\n",
        "# =======================\n",
        "X = df.loc[y_final.index].select_dtypes(include=[np.number]).drop(columns=[\"ITCRM\"], errors='ignore')\n",
        "\n",
        "# Limpiar variables con muchos NaN\n",
        "umbral_nan = 0.3\n",
        "X_clean = X.dropna(axis=1, thresh=int(len(X) * (1 - umbral_nan)))\n",
        "\n",
        "# Imputar NaN restantes\n",
        "from sklearn.impute import SimpleImputer\n",
        "imputer = SimpleImputer(strategy='median')\n",
        "X_imputed = pd.DataFrame(imputer.fit_transform(X_clean),\n",
        "                        columns=X_clean.columns,\n",
        "                        index=X_clean.index)\n",
        "\n",
        "print(f\"\\n=== DATOS FINALES ===\")\n",
        "print(f\"Observaciones: {len(X_imputed)}\")\n",
        "print(f\"Variables predictoras: {len(X_imputed.columns)}\")\n",
        "print(f\"Serie: {metodo}\")\n",
        "\n",
        "# =======================\n",
        "# 4. Validaci√≥n cruzada temporal para TODOS los modelos\n",
        "# =======================\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "import catboost as cb\n",
        "\n",
        "# Configurar modelos optimizados\n",
        "modelos = {\n",
        "    \"RandomForest\": RandomForestRegressor(\n",
        "        n_estimators=200, max_depth=8, min_samples_split=15,\n",
        "        min_samples_leaf=10, random_state=42, n_jobs=-1\n",
        "    ),\n",
        "    \"XGBoost\": xgb.XGBRegressor(\n",
        "        n_estimators=200, max_depth=6, learning_rate=0.1,\n",
        "        random_state=42, n_jobs=-1\n",
        "    ),\n",
        "    \"LightGBM\": lgb.LGBMRegressor(\n",
        "        n_estimators=200, max_depth=6, learning_rate=0.1,\n",
        "        random_state=42, n_jobs=-1, verbosity=-1\n",
        "    ),\n",
        "    \"CatBoost\": cb.CatBoostRegressor(\n",
        "        iterations=200, depth=6, learning_rate=0.1,\n",
        "        random_state=42, verbose=0\n",
        "    )\n",
        "}\n",
        "\n",
        "# Validaci√≥n cruzada temporal\n",
        "n_splits = 5\n",
        "tscv = TimeSeriesSplit(n_splits=n_splits, test_size=int(len(X_imputed) * 0.2))\n",
        "\n",
        "resultados_cv = {nombre: [] for nombre in modelos.keys()}\n",
        "\n",
        "print(f\"\\n=== VALIDACI√ìN CRUZADA TEMPORAL ({n_splits} folds) ===\")\n",
        "\n",
        "for fold, (train_idx, test_idx) in enumerate(tscv.split(X_imputed)):\n",
        "    print(f\"\\n--- FOLD {fold + 1}/{n_splits} ---\")\n",
        "\n",
        "    X_train, X_test = X_imputed.iloc[train_idx], X_imputed.iloc[test_idx]\n",
        "    y_train, y_test = y_final.iloc[train_idx], y_final.iloc[test_idx]\n",
        "\n",
        "    for nombre, modelo in modelos.items():\n",
        "        try:\n",
        "            # Entrenar y predecir\n",
        "            modelo.fit(X_train, y_train)\n",
        "            y_pred = modelo.predict(X_test)\n",
        "\n",
        "            # M√©tricas\n",
        "            mae = mean_absolute_error(y_test, y_pred)\n",
        "            rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "            r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "            resultados_cv[nombre].append({\n",
        "                \"fold\": fold + 1, \"MAE\": mae, \"RMSE\": rmse, \"R2\": r2\n",
        "            })\n",
        "\n",
        "            print(f\"{nombre:12} - RMSE: {rmse:.4f}, R¬≤: {r2:.4f}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error en {nombre}: {e}\")\n",
        "\n",
        "# =======================\n",
        "# 5. Entrenamiento FINAL de TODOS los modelos\n",
        "# =======================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ENTRENAMIENTO FINAL DE TODOS LOS MODELOS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "modelos_entrenados = {}\n",
        "predicciones_in_sample = {}\n",
        "importancias_modelos = {}\n",
        "\n",
        "for nombre, modelo in modelos.items():\n",
        "    print(f\"Entrenando {nombre}...\")\n",
        "    modelo.fit(X_imputed, y_final)\n",
        "    modelos_entrenados[nombre] = modelo\n",
        "\n",
        "    # Predicciones in-sample\n",
        "    y_pred = modelo.predict(X_imputed)\n",
        "    predicciones_in_sample[nombre] = y_pred\n",
        "\n",
        "    # Importancia de variables para cada modelo\n",
        "    try:\n",
        "        if nombre == \"CatBoost\":\n",
        "            importancias = modelo.get_feature_importance()\n",
        "        elif nombre == \"XGBoost\":\n",
        "            importancias = modelo.feature_importances_\n",
        "        elif nombre == \"LightGBM\":\n",
        "            importancias = modelo.feature_importances_\n",
        "        elif nombre == \"RandomForest\":\n",
        "            importancias = modelo.feature_importances_\n",
        "        else:\n",
        "            importancias = np.zeros(len(X_imputed.columns))\n",
        "\n",
        "        importancias_modelos[nombre] = pd.DataFrame({\n",
        "            'Variable': X_imputed.columns,\n",
        "            'Importancia': importancias\n",
        "        }).sort_values('Importancia', ascending=False)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error calculando importancia para {nombre}: {e}\")\n",
        "        importancias_modelos[nombre] = pd.DataFrame()\n",
        "\n",
        "# =======================\n",
        "# 6. An√°lisis de resultados de TODOS los modelos\n",
        "# =======================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"RESULTADOS PROMEDIO - TODOS LOS MODELOS (Validaci√≥n Cruzada)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "resultados_promedio = {}\n",
        "for modelo, folds in resultados_cv.items():\n",
        "    if folds:\n",
        "        df_folds = pd.DataFrame(folds)\n",
        "        avg = df_folds.mean()\n",
        "        std = df_folds.std()\n",
        "\n",
        "        resultados_promedio[modelo] = {\n",
        "            'MAE_avg': avg['MAE'], 'MAE_std': std['MAE'],\n",
        "            'RMSE_avg': avg['RMSE'], 'RMSE_std': std['RMSE'],\n",
        "            'R2_avg': avg['R2'], 'R2_std': std['R2']\n",
        "        }\n",
        "\n",
        "        print(f\"\\n{modelo}:\")\n",
        "        print(f\"  RMSE: {avg['RMSE']:.4f} (¬±{std['RMSE']:.4f})\")\n",
        "        print(f\"  R¬≤:   {avg['R2']:.4f} (¬±{std['R2']:.4f})\")\n",
        "\n",
        "# Seleccionar mejor modelo\n",
        "mejor_modelo_nombre = min(resultados_promedio.items(),\n",
        "                         key=lambda x: x[1]['RMSE_avg'])[0]\n",
        "mejor_modelo = modelos_entrenados[mejor_modelo_nombre]\n",
        "print(f\"\\nüéØ MEJOR MODELO: {mejor_modelo_nombre}\")\n",
        "\n",
        "# =======================\n",
        "# 7. C√ÅLCULO DE INTERVALOS DE CONFIANZA para el MEJOR MODELO\n",
        "# =======================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"C√ÅLCULO DE INTERVALOS DE CONFIANZA - MEJOR MODELO\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "def calcular_intervalos_confianza(modelo, X, y_real, metodo='residual', n_bootstraps=1000, alpha=0.05):\n",
        "    \"\"\"\n",
        "    Calcula intervalos de confianza usando diferentes m√©todos\n",
        "    \"\"\"\n",
        "    print(f\"M√©todo utilizado: {metodo}\")\n",
        "\n",
        "    # Predicci√≥n puntual\n",
        "    y_pred = modelo.predict(X)\n",
        "    residuales = y_real - y_pred\n",
        "\n",
        "    if metodo == 'residual':\n",
        "        # M√©todo 1: Basado en residuales (aproximado)\n",
        "        std_residual = np.std(residuales)\n",
        "        z_score = 1.96  # Para 95% de confianza\n",
        "\n",
        "        ic_inferior = y_pred - z_score * std_residual\n",
        "        ic_superior = y_pred + z_score * std_residual\n",
        "\n",
        "        print(f\"Desviaci√≥n est√°ndar de residuales: {std_residual:.4f}\")\n",
        "\n",
        "    elif metodo == 'bootstrap':\n",
        "        # M√©todo 2: Bootstrap (m√°s robusto)\n",
        "        print(f\"Ejecutando bootstrap con {n_bootstraps} iteraciones...\")\n",
        "        predicciones_bootstrap = []\n",
        "\n",
        "        for i in range(n_bootstraps):\n",
        "            # Remuestreo con reemplazo\n",
        "            indices = np.random.choice(len(X), len(X), replace=True)\n",
        "            X_boot = X.iloc[indices]\n",
        "            y_boot = y_real.iloc[indices]\n",
        "\n",
        "            # Re-entrenar modelo (copia para no afectar el original)\n",
        "            modelo_boot = clone_modelo(modelo)\n",
        "            modelo_boot.fit(X_boot, y_boot)\n",
        "\n",
        "            # Predecir en datos originales\n",
        "            y_pred_boot = modelo_boot.predict(X)\n",
        "            predicciones_bootstrap.append(y_pred_boot)\n",
        "\n",
        "        predicciones_bootstrap = np.array(predicciones_bootstrap)\n",
        "        ic_inferior = np.percentile(predicciones_bootstrap, (alpha/2)*100, axis=0)\n",
        "        ic_superior = np.percentile(predicciones_bootstrap, (1-alpha/2)*100, axis=0)\n",
        "\n",
        "    else:\n",
        "        raise ValueError(\"M√©todo no reconocido\")\n",
        "\n",
        "    # Calcular cobertura emp√≠rica (si tenemos datos reales)\n",
        "    cobertura = np.mean((y_real >= ic_inferior) & (y_real <= ic_superior))\n",
        "    ancho_promedio = np.mean(ic_superior - ic_inferior)\n",
        "\n",
        "    print(f\"Cobertura emp√≠rica del IC: {cobertura:.3f}\")\n",
        "    print(f\"Ancho promedio del IC: {ancho_promedio:.4f}\")\n",
        "\n",
        "    return y_pred, ic_inferior, ic_superior, residuales\n",
        "\n",
        "def clone_modelo(modelo_original):\n",
        "    \"\"\"Crea una copia del modelo para bootstrap\"\"\"\n",
        "    if hasattr(modelo_original, 'copy'):\n",
        "        return modelo_original.copy()\n",
        "    else:\n",
        "        # Para modelos que no tienen m√©todo copy, recrear con mismos par√°metros\n",
        "        if isinstance(modelo_original, RandomForestRegressor):\n",
        "            return RandomForestRegressor(**modelo_original.get_params())\n",
        "        elif isinstance(modelo_original, xgb.XGBRegressor):\n",
        "            return xgb.XGBRegressor(**modelo_original.get_params())\n",
        "        elif isinstance(modelo_original, lgb.LGBMRegressor):\n",
        "            return lgb.LGBMRegressor(**modelo_original.get_params())\n",
        "        elif isinstance(modelo_original, cb.CatBoostRegressor):\n",
        "            return cb.CatBoostRegressor(**modelo_original.get_params())\n",
        "        else:\n",
        "            return modelo_original\n",
        "\n",
        "# Calcular intervalos de confianza para el mejor modelo\n",
        "y_pred_mejor, ic_inf, ic_sup, residuales_mejor = calcular_intervalos_confianza(\n",
        "    mejor_modelo, X_imputed, y_final, metodo='residual'\n",
        ")\n",
        "\n",
        "# =======================\n",
        "# 8. Pron√≥stico fuera de muestra con IC para el MEJOR MODELO\n",
        "# =======================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"PRON√ìSTICO FUERA DE MUESTRA CON IC - MEJOR MODELO\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "predicciones_out_sample = {}\n",
        "ic_out_sample = {}\n",
        "\n",
        "# Crear conjunto para pron√≥stico fuera de muestra (√∫ltimos 74 meses)\n",
        "if len(X_imputed) > 74:\n",
        "    X_train_final = X_imputed.iloc[:-74]\n",
        "    y_train_final = y_final.iloc[:-74]\n",
        "    X_test_final = X_imputed.iloc[-74:]\n",
        "    y_test_final = y_final.iloc[-74:]\n",
        "\n",
        "    # Re-entrenar mejor modelo con datos hasta el punto de corte\n",
        "    mejor_modelo.fit(X_train_final, y_train_final)\n",
        "\n",
        "    # Pron√≥stico puntual\n",
        "    y_pred_out = mejor_modelo.predict(X_test_final)\n",
        "    predicciones_out_sample[mejor_modelo_nombre] = y_pred_out\n",
        "\n",
        "    # Calcular IC para pron√≥stico fuera de muestra\n",
        "    residuales_train = y_train_final - mejor_modelo.predict(X_train_final)\n",
        "    std_residual_out = np.std(residuales_train)\n",
        "\n",
        "    ic_inf_out = y_pred_out - 1.96 * std_residual_out\n",
        "    ic_sup_out = y_pred_out + 1.96 * std_residual_out\n",
        "\n",
        "    ic_out_sample[mejor_modelo_nombre] = {\n",
        "        'inferior': ic_inf_out,\n",
        "        'superior': ic_sup_out\n",
        "    }\n",
        "\n",
        "    print(f\"Pron√≥stico fuera de muestra calculado para {len(y_pred_out)} meses\")\n",
        "    print(f\"Desviaci√≥n est√°ndar residuales (train): {std_residual_out:.4f}\")\n",
        "\n",
        "else:\n",
        "    print(\"Datos insuficientes para pron√≥stico fuera de muestra\")\n",
        "\n",
        "# =======================\n",
        "# 9. AN√ÅLISIS SHAP solo del MEJOR modelo\n",
        "# =======================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"AN√ÅLISIS SHAP - SOLO MEJOR MODELO\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "shap_importance = pd.DataFrame()\n",
        "lime_df = pd.DataFrame()\n",
        "\n",
        "try:\n",
        "    import shap\n",
        "\n",
        "    # Calcular valores SHAP\n",
        "    explainer = shap.TreeExplainer(mejor_modelo)\n",
        "    shap_values = explainer.shap_values(X_imputed)\n",
        "\n",
        "    # Importancia global\n",
        "    shap_importance = pd.DataFrame({\n",
        "        'Variable': X_imputed.columns,\n",
        "        'SHAP_Importance': np.abs(shap_values).mean(axis=0)\n",
        "    }).sort_values('SHAP_Importance', ascending=False)\n",
        "\n",
        "    print(\"\\nTop 15 Variables M√°s Importantes (SHAP) - Mejor Modelo:\")\n",
        "    for i, row in shap_importance.head(15).iterrows():\n",
        "        print(f\"  {i+1:2d}. {row['Variable']:30} {row['SHAP_Importance']:.6f}\")\n",
        "\n",
        "    # Gr√°fico SHAP\n",
        "    plt.figure(figsize=(12, 8))\n",
        "    shap.summary_plot(shap_values, X_imputed, plot_type=\"bar\", show=False)\n",
        "    plt.title(f'SHAP Feature Importance - {mejor_modelo_nombre}', fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('shap_importance.png', dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "    print(\"‚úÖ Gr√°fico SHAP guardado como 'shap_importance.png'\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error en SHAP: {e}\")\n",
        "\n",
        "# =======================\n",
        "# 10. AN√ÅLISIS LIME solo del MEJOR modelo\n",
        "# =======================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"AN√ÅLISIS LIME - SOLO MEJOR MODELO\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "try:\n",
        "    from lime import lime_tabular\n",
        "\n",
        "    # Preparar explainer LIME\n",
        "    lime_explainer = lime_tabular.LimeTabularExplainer(\n",
        "        training_data=np.array(X_imputed),\n",
        "        feature_names=X_imputed.columns.tolist(),\n",
        "        mode='regression',\n",
        "        verbose=False,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # Analizar las primeras 3 observaciones\n",
        "    lime_results = []\n",
        "    n_explicaciones = min(3, len(X_imputed))\n",
        "\n",
        "    for i in range(n_explicaciones):\n",
        "        exp = lime_explainer.explain_instance(\n",
        "            data_row=X_imputed.iloc[i].values,\n",
        "            predict_fn=mejor_modelo.predict,\n",
        "            num_features=10\n",
        "        )\n",
        "\n",
        "        # Convertir explicaci√≥n a DataFrame\n",
        "        explanation_df = pd.DataFrame(exp.as_list(), columns=['Variable', 'Impacto'])\n",
        "        explanation_df['Observacion'] = i\n",
        "        explanation_df['Fecha'] = X_imputed.index[i]\n",
        "        explanation_df['Modelo'] = mejor_modelo_nombre\n",
        "        lime_results.append(explanation_df)\n",
        "\n",
        "        print(f\"\\nObservaci√≥n {i+1} ({X_imputed.index[i].strftime('%Y-%m')}):\")\n",
        "        for _, row in explanation_df.head(5).iterrows():\n",
        "            print(f\"  {row['Variable']:30} {row['Impacto']:+.6f}\")\n",
        "\n",
        "    lime_df = pd.concat(lime_results, ignore_index=True)\n",
        "    print(\"‚úÖ An√°lisis LIME completado\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error en LIME: {e}\")\n",
        "\n",
        "# =======================\n",
        "# 11. GR√ÅFICOS COMPLETOS con INTERVALOS DE CONFIANZA\n",
        "# =======================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"GENERANDO GR√ÅFICOS CON INTERVALOS DE CONFIANZA\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Gr√°fico 1: Serie real vs predicciones con IC del mejor modelo\n",
        "plt.figure(figsize=(16, 12))\n",
        "\n",
        "# Subplot 1: Serie real vs predicciones con IC (in-sample)\n",
        "plt.subplot(2, 2, 1)\n",
        "plt.plot(y_final.index, y_final.values, label='Real', linewidth=2, alpha=0.9, color='black')\n",
        "plt.plot(y_final.index, y_pred_mejor, label=f'Predicho ({mejor_modelo_nombre})',\n",
        "         linewidth=1.5, alpha=0.8, color='red')\n",
        "\n",
        "# Rellenar √°rea del intervalo de confianza\n",
        "plt.fill_between(y_final.index, ic_inf, ic_sup, alpha=0.3, color='red',\n",
        "                label='IC 95%')\n",
        "\n",
        "plt.title(f'Serie Real vs Predicciones con IC\\n({mejor_modelo_nombre})', fontsize=12)\n",
        "plt.xlabel('Fecha')\n",
        "plt.ylabel('ITCRM')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Subplot 2: Zoom a los √∫ltimos 24 meses con IC\n",
        "plt.subplot(2, 2, 2)\n",
        "ultimos_meses = 24\n",
        "if len(y_final) > ultimos_meses:\n",
        "    idx_ultimos = slice(-ultimos_meses, None)\n",
        "    plt.plot(y_final.index[idx_ultimos], y_final.values[idx_ultimos],\n",
        "             label='Real', linewidth=2, alpha=0.9, color='black', marker='o')\n",
        "    plt.plot(y_final.index[idx_ultimos], y_pred_mejor[idx_ultimos],\n",
        "             label=f'Predicho', linewidth=1.5, alpha=0.8, color='red', marker='s')\n",
        "    plt.fill_between(y_final.index[idx_ultimos], ic_inf[idx_ultimos],\n",
        "                    ic_sup[idx_ultimos], alpha=0.3, color='red', label='IC 95%')\n",
        "\n",
        "    plt.title(f'Zoom √∫ltimos {ultimos_meses} meses con IC', fontsize=12)\n",
        "    plt.xlabel('Fecha')\n",
        "    plt.ylabel('ITCRM')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Subplot 3: Pron√≥stico fuera de muestra con IC\n",
        "if predicciones_out_sample:\n",
        "    plt.subplot(2, 2, 3)\n",
        "    y_pred_out = predicciones_out_sample[mejor_modelo_nombre]\n",
        "    ic_inf_out = ic_out_sample[mejor_modelo_nombre]['inferior']\n",
        "    ic_sup_out = ic_out_sample[mejor_modelo_nombre]['superior']\n",
        "\n",
        "    plt.plot(y_test_final.index, y_test_final.values, label='Real',\n",
        "             linewidth=2, alpha=0.9, color='black', marker='o')\n",
        "    plt.plot(y_test_final.index, y_pred_out, label='Predicho',\n",
        "             linewidth=1.5, alpha=0.8, color='blue', marker='s')\n",
        "    plt.fill_between(y_test_final.index, ic_inf_out, ic_sup_out,\n",
        "                    alpha=0.3, color='blue', label='IC 95%')\n",
        "\n",
        "    plt.title('Pron√≥stico Fuera de Muestra con IC', fontsize=12)\n",
        "    plt.xlabel('Fecha')\n",
        "    plt.ylabel('ITCRM')\n",
        "    plt.legend()\n",
        "    plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Subplot 4: Distribuci√≥n de residuales y cobertura IC\n",
        "plt.subplot(2, 2, 4)\n",
        "# Histograma de residuales\n",
        "plt.hist(residuales_mejor, bins=30, alpha=0.7, color='green', edgecolor='black',\n",
        "         density=True, label='Residuales')\n",
        "\n",
        "# L√≠neas para ¬±1.96œÉ (l√≠mites del IC)\n",
        "std_residual = np.std(residuales_mejor)\n",
        "plt.axvline(-1.96 * std_residual, color='red', linestyle='--',\n",
        "           label='¬±1.96œÉ (l√≠mites IC)')\n",
        "plt.axvline(1.96 * std_residual, color='red', linestyle='--')\n",
        "\n",
        "plt.title('Distribuci√≥n Residuales y L√≠mites IC', fontsize=12)\n",
        "plt.xlabel('Residual')\n",
        "plt.ylabel('Densidad')\n",
        "plt.legend()\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('resultados_con_ic.png', dpi=300, bbox_inches='tight')\n",
        "plt.close()\n",
        "print(\"‚úÖ Gr√°fico con intervalos de confianza guardado como 'resultados_con_ic.png'\")\n",
        "\n",
        "# =======================\n",
        "# 12. GUARDAR RESULTADOS COMPLETOS EN EXCEL\n",
        "# =======================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"GUARDANDO RESULTADOS COMPLETOS EN EXCEL\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "with pd.ExcelWriter('resultados_completos_con_ic.xlsx') as writer:\n",
        "    # 1. Metadatos del an√°lisis\n",
        "    metadata = pd.DataFrame({\n",
        "        'Par√°metro': ['Fecha an√°lisis', 'Total observaciones', 'Variables predictoras',\n",
        "                     'Mejor modelo', 'Transformaci√≥n', 'Folds CV', 'M√©todo IC', 'Cobertura IC'],\n",
        "        'Valor': [datetime.now().strftime('%Y-%m-%d %H:%M'), len(X_imputed),\n",
        "                 len(X_imputed.columns), mejor_modelo_nombre, metodo, n_splits,\n",
        "                 'Residual', f\"{np.mean((y_final >= ic_inf) & (y_final <= ic_sup)):.3f}\"]\n",
        "    })\n",
        "    metadata.to_excel(writer, sheet_name='Metadatos', index=False)\n",
        "\n",
        "    # 2. Resultados de validaci√≥n cruzada (todos los modelos)\n",
        "    resultados_cv_df = pd.DataFrame([\n",
        "        {**{'modelo': model}, **metrics}\n",
        "        for model, metrics in resultados_promedio.items()\n",
        "    ])\n",
        "    resultados_cv_df.to_excel(writer, sheet_name='Resultados_CV', index=False)\n",
        "\n",
        "    # 3. Predicciones con IC del mejor modelo (in-sample)\n",
        "    predicciones_ic_df = pd.DataFrame({\n",
        "        'Fecha': y_final.index,\n",
        "        'Real': y_final.values,\n",
        "        'Predicho': y_pred_mejor,\n",
        "        'IC_Inferior': ic_inf,\n",
        "        'IC_Superior': ic_sup,\n",
        "        'Residual': residuales_mejor,\n",
        "        'Dentro_IC': (y_final >= ic_inf) & (y_final <= ic_sup)\n",
        "    })\n",
        "    predicciones_ic_df.to_excel(writer, sheet_name='Predicciones_IC_InSample', index=False)\n",
        "\n",
        "    # 4. Pron√≥stico fuera de muestra con IC\n",
        "    if predicciones_out_sample:\n",
        "        predicciones_os_df = pd.DataFrame({\n",
        "            'Fecha': y_test_final.index,\n",
        "            'Real': y_test_final.values,\n",
        "            'Predicho': predicciones_out_sample[mejor_modelo_nombre],\n",
        "            'IC_Inferior': ic_out_sample[mejor_modelo_nombre]['inferior'],\n",
        "            'IC_Superior': ic_out_sample[mejor_modelo_nombre]['superior'],\n",
        "            'Residual': y_test_final.values - predicciones_out_sample[mejor_modelo_nombre]\n",
        "        })\n",
        "        predicciones_os_df.to_excel(writer, sheet_name='Predicciones_FueraMuestra', index=False)\n",
        "\n",
        "    # 5. Importancia de variables (todos los modelos)\n",
        "    for nombre, df_importancia in importancias_modelos.items():\n",
        "        if not df_importancia.empty:\n",
        "            df_importancia.to_excel(writer, sheet_name=f'Importancia_{nombre}', index=False)\n",
        "\n",
        "    # 6. Resultados SHAP\n",
        "    if not shap_importance.empty:\n",
        "        shap_importance.to_excel(writer, sheet_name='SHAP_Importance', index=False)\n",
        "\n",
        "    # 7. Resultados LIME\n",
        "    if not lime_df.empty:\n",
        "        lime_df.to_excel(writer, sheet_name='LIME_Explicaciones', index=False)\n",
        "\n",
        "    # 8. Datos originales procesados\n",
        "    datos_procesados = pd.DataFrame({\n",
        "        'Fecha': X_imputed.index,\n",
        "        'ITCRM': y_final.values\n",
        "    })\n",
        "    datos_procesados = pd.concat([datos_procesados, X_imputed], axis=1)\n",
        "    datos_procesados.to_excel(writer, sheet_name='Datos_Procesados', index=False)\n",
        "\n",
        "print(\"‚úÖ Archivo Excel guardado como 'resultados_completos_con_ic.xlsx'\")\n",
        "\n",
        "# =======================\n",
        "# 13. RESUMEN FINAL\n",
        "# =======================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"RESUMEN FINAL DEL AN√ÅLISIS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "print(f\"üìä Mejor modelo seleccionado: {mejor_modelo_nombre}\")\n",
        "print(f\"üìà M√©tricas promedio (CV):\")\n",
        "print(f\"   - RMSE: {resultados_promedio[mejor_modelo_nombre]['RMSE_avg']:.4f}\")\n",
        "print(f\"   - R¬≤: {resultados_promedio[mejor_modelo_nombre]['R2_avg']:.4f}\")\n",
        "print(f\"üìê Intervalos de confianza: 95% (m√©todo residual)\")\n",
        "print(f\"   - Cobertura emp√≠rica: {np.mean((y_final >= ic_inf) & (y_final <= ic_sup)):.3f}\")\n",
        "print(f\"   - Ancho promedio: {np.mean(ic_sup - ic_inf):.4f}\")\n",
        "print(f\"üíæ Resultados guardados:\")\n",
        "print(f\"   - resultados_completos_con_ic.xlsx (datos completos)\")\n",
        "print(f\"   - resultados_con_ic.png (gr√°ficos)\")\n",
        "print(f\"   - shap_importance.png (importancia SHAP)\")\n",
        "\n",
        "print(\"\\nüéØ AN√ÅLISIS COMPLETADO EXITOSAMENTE\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ByTNpfKuasHL",
        "outputId": "c93d7bf0-eac7-4777-de4e-1871a1038538"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "=== INFORMACI√ìN DE LOS DATOS ===\n",
            "Dimensi√≥n original: (370, 446)\n",
            "Dimensi√≥n despu√©s de limpiar NaN: (370, 446)\n",
            "√çndice temporal creado autom√°ticamente\n",
            "Rango temporal: 1993-08-31 00:00:00 a 2024-05-31 00:00:00\n",
            "\n",
            "ITCRM Original:\n",
            "  p-value: 0.057772\n",
            "  Estacionaria: NO\n",
            "\n",
            "‚ö†Ô∏è  Aplicando diferenciaci√≥n para lograr estacionariedad...\n",
            "\n",
            "ITCRM Diferenciada:\n",
            "  p-value: 0.000000\n",
            "  Estacionaria: S√ç\n",
            "‚úÖ Usando serie diferenciada (estacionaria)\n",
            "\n",
            "=== DATOS FINALES ===\n",
            "Observaciones: 369\n",
            "Variables predictoras: 444\n",
            "Serie: diferenciada\n",
            "\n",
            "=== VALIDACI√ìN CRUZADA TEMPORAL (5 folds) ===\n",
            "\n",
            "--- FOLD 1/5 ---\n",
            "RandomForest - RMSE: 0.0238, R¬≤: -0.9479\n",
            "XGBoost      - RMSE: 0.0216, R¬≤: -0.6063\n",
            "LightGBM     - RMSE: 0.0236, R¬≤: -0.9130\n",
            "CatBoost     - RMSE: 0.0234, R¬≤: -0.8932\n",
            "\n",
            "--- FOLD 2/5 ---\n",
            "RandomForest - RMSE: 0.0251, R¬≤: -0.0075\n",
            "XGBoost      - RMSE: 0.0261, R¬≤: -0.0868\n",
            "LightGBM     - RMSE: 0.0261, R¬≤: -0.0836\n",
            "CatBoost     - RMSE: 0.0247, R¬≤: 0.0245\n",
            "\n",
            "--- FOLD 3/5 ---\n",
            "RandomForest - RMSE: 0.0262, R¬≤: 0.0360\n",
            "XGBoost      - RMSE: 0.0289, R¬≤: -0.1699\n",
            "LightGBM     - RMSE: 0.0263, R¬≤: 0.0277\n",
            "CatBoost     - RMSE: 0.0257, R¬≤: 0.0746\n",
            "\n",
            "--- FOLD 4/5 ---\n",
            "RandomForest - RMSE: 0.0205, R¬≤: 0.0462\n",
            "XGBoost      - RMSE: 0.0216, R¬≤: -0.0609\n",
            "LightGBM     - RMSE: 0.0223, R¬≤: -0.1277\n",
            "CatBoost     - RMSE: 0.0208, R¬≤: 0.0113\n",
            "\n",
            "--- FOLD 5/5 ---\n",
            "RandomForest - RMSE: 0.0239, R¬≤: 0.0716\n",
            "XGBoost      - RMSE: 0.0244, R¬≤: 0.0280\n",
            "LightGBM     - RMSE: 0.0238, R¬≤: 0.0759\n",
            "CatBoost     - RMSE: 0.0243, R¬≤: 0.0409\n",
            "\n",
            "============================================================\n",
            "ENTRENAMIENTO FINAL DE TODOS LOS MODELOS\n",
            "============================================================\n",
            "Entrenando RandomForest...\n",
            "Entrenando XGBoost...\n",
            "Entrenando LightGBM...\n",
            "Entrenando CatBoost...\n",
            "\n",
            "============================================================\n",
            "RESULTADOS PROMEDIO - TODOS LOS MODELOS (Validaci√≥n Cruzada)\n",
            "============================================================\n",
            "\n",
            "RandomForest:\n",
            "  RMSE: 0.0239 (¬±0.0022)\n",
            "  R¬≤:   -0.1603 (¬±0.4412)\n",
            "\n",
            "XGBoost:\n",
            "  RMSE: 0.0245 (¬±0.0031)\n",
            "  R¬≤:   -0.1792 (¬±0.2490)\n",
            "\n",
            "LightGBM:\n",
            "  RMSE: 0.0244 (¬±0.0017)\n",
            "  R¬≤:   -0.2041 (¬±0.4047)\n",
            "\n",
            "CatBoost:\n",
            "  RMSE: 0.0238 (¬±0.0018)\n",
            "  R¬≤:   -0.1484 (¬±0.4170)\n",
            "\n",
            "üéØ MEJOR MODELO: CatBoost\n",
            "\n",
            "============================================================\n",
            "C√ÅLCULO DE INTERVALOS DE CONFIANZA - MEJOR MODELO\n",
            "============================================================\n",
            "M√©todo utilizado: residual\n",
            "Desviaci√≥n est√°ndar de residuales: 0.0014\n",
            "Cobertura emp√≠rica del IC: 0.959\n",
            "Ancho promedio del IC: 0.0056\n",
            "\n",
            "============================================================\n",
            "PRON√ìSTICO FUERA DE MUESTRA CON IC - MEJOR MODELO\n",
            "============================================================\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}